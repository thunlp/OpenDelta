02/28/2022 10:01:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
02/28/2022 10:01:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
compute_memory=False,
compute_time=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_test=True,
do_train=True,
eval_accumulation_steps=None,
eval_steps=200,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
is_seq2seq=True,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.023938918670661075,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=outputs_search/lowrankadapter.superglue-boolq.t5-base/926759/runs/Feb28_10-01-22_node4,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=5000,
metric_for_best_model=average_metrics,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
output_dir=outputs_search/lowrankadapter.superglue-boolq.t5-base/926759,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=64,
predict_with_generate=True,
prediction_loss_only=False,
print_num_parameters=False,
push_to_hub=False,
push_to_hub_model_id=926759,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=outputs_search/lowrankadapter.superglue-boolq.t5-base/926759,
save_on_each_node=False,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=1,
seed=100,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
split_validation_test=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=500,
weight_decay=0.0,
)
loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/hushengding/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.10.0",
  "use_cache": true,
  "vocab_size": 32128
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/hushengding/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.10.0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/hushengding/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d
loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/hushengding/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529
loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None
loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/hushengding/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.10.0",
  "use_cache": true,
  "vocab_size": 32128
}

loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/hushengding/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4
All model checkpoint weights were used when initializing T5ForConditionalGeneration.

All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
[WARNING|(OpenDelta)delta_configs:200]2022-02-28 10:01:36,527 >> The following keys are not used by <class 'opendelta.delta_models.low_rank_adapter.LowRankAdapterConfig'>.__init__ function: ['description', 'argument_default', 'prefix_chars', 'conflict_handler', '_registries', '_actions', '_option_string_actions', '_action_groups', '_mutually_exclusive_groups', '_defaults', '_negative_number_matcher', '_has_negative_number_optionals', 'prog', 'usage', 'epilog', 'formatter_class', 'fromfile_prefix_chars', 'add_help', 'allow_abbrev', '_positionals', '_optionals', '_subparsers', 'batch_size']
[INFO|(OpenDelta)delta_configs:214]2022-02-28 10:01:36,528 >> Model config LowRankAdapterConfig {
  "low_rank_rank": 1,
  "low_rank_w_init": "glorot-uniform",
  "non_linearity": "gelu_new",
  "opendelta_version": "0.0.1",
  "reduction_factor": 32,
  "transformers_version": "4.10.0",
  "unfrozen_modules": [
    "deltas",
    "layer_norm",
    "final_layer_norm"
  ]
}

[INFO|(OpenDelta)structure_mapping:321]2022-02-28 10:01:36,528 >> Since you are using the common structure mapping, draw the transformed parameter structure for checking.
root                                                                            
â”œâ”€â”€ embeddings (Embedding) weight:[32100, 768]                                  
â”œâ”€â”€ encoder (T5Stack)                                                           
â”‚   â”œâ”€â”€ embeddings (Embedding) weight:[32100, 768]                              
â”‚   â”œâ”€â”€ block (ModuleList)                                                      
â”‚   â”‚   â”œâ”€â”€ 0 (T5Block)                                                         
â”‚   â”‚   â”‚   â”œâ”€â”€ attn (T5LayerSelfAttention)                                     
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q,k,v,proj(Linear) weight:[768, 768]                        
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SelfAttention.relative_attention_bias (Embedding)           
â”‚   â”‚   â”‚   â”‚   â”‚   weight:[32, 12]                                             
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚   â”‚   â””â”€â”€ ff (T5LayerFF)                                                  
â”‚   â”‚   â”‚       â”œâ”€â”€ w1 (Linear) weight:[3072, 768]                              
â”‚   â”‚   â”‚       â”œâ”€â”€ w2 (Linear) weight:[768, 3072]                              
â”‚   â”‚   â”‚       â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚   â””â”€â”€ 1-11(T5Block)                                                       
â”‚   â”‚       â”œâ”€â”€ attn (T5LayerSelfAttention)                                     
â”‚   â”‚       â”‚   â”œâ”€â”€ q,k,v,proj(Linear) weight:[768, 768]                        
â”‚   â”‚       â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚       â””â”€â”€ ff (T5LayerFF)                                                  
â”‚   â”‚           â”œâ”€â”€ w1 (Linear) weight:[3072, 768]                              
â”‚   â”‚           â”œâ”€â”€ w2 (Linear) weight:[768, 3072]                              
â”‚   â”‚           â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                                   
â”œâ”€â”€ decoder (T5Stack)                                                           
â”‚   â”œâ”€â”€ embeddings (Embedding) weight:[32100, 768]                              
â”‚   â”œâ”€â”€ block (ModuleList)                                                      
â”‚   â”‚   â”œâ”€â”€ 0 (T5Block)                                                         
â”‚   â”‚   â”‚   â”œâ”€â”€ attn (T5LayerSelfAttention)                                     
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q,k,v,proj(Linear) weight:[768, 768]                        
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SelfAttention.relative_attention_bias (Embedding)           
â”‚   â”‚   â”‚   â”‚   â”‚   weight:[32, 12]                                             
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚   â”‚   â”œâ”€â”€ crossattn (T5LayerCrossAttention)                               
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q,k,v,proj(Linear) weight:[768, 768]                        
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚   â”‚   â””â”€â”€ ff (T5LayerFF)                                                  
â”‚   â”‚   â”‚       â”œâ”€â”€ w1 (Linear) weight:[3072, 768]                              
â”‚   â”‚   â”‚       â”œâ”€â”€ w2 (Linear) weight:[768, 3072]                              
â”‚   â”‚   â”‚       â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚   â””â”€â”€ 1-11(T5Block)                                                       
â”‚   â”‚       â”œâ”€â”€ attn(T5LayerSelfAttention),crossattn(T5LayerCrossAttention)     
â”‚   â”‚       â”‚   â”œâ”€â”€ q,k,v,proj(Linear) weight:[768, 768]                        
â”‚   â”‚       â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â”‚       â””â”€â”€ ff (T5LayerFF)                                                  
â”‚   â”‚           â”œâ”€â”€ w1 (Linear) weight:[3072, 768]                              
â”‚   â”‚           â”œâ”€â”€ w2 (Linear) weight:[768, 3072]                              
â”‚   â”‚           â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                       
â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                                   
â””â”€â”€ lm_head (Virtual)                                                           
    â””â”€â”€ proj (Linear) weight:[32100, 768]                                       
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:36,738 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:36,900 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:37,165 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:37,314 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:37,591 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:37,745 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:38,020 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:38,174 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:38,398 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:38,551 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:38,794 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:38,943 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:39,180 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:39,328 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:39,567 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:39,726 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:39,985 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:40,141 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:40,373 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:40,536 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:40,781 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:40,926 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:41,174 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:41,323 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:41,559 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:41,864 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:42,117 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:42,478 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:42,744 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:43,109 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:43,368 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:43,676 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:43,919 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:44,258 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:44,472 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:44,801 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:45,052 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:45,455 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:45,818 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:46,184 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:46,446 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:46,801 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:47,052 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:47,386 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:47,632 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:47,964 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:48,201 >> Got hidden dim hidden_dim 768
[DEBUG|(OpenDelta)low_rank_adapter:90]2022-02-28 10:01:48,577 >> Got hidden dim hidden_dim 768
root                                                                            
â”œâ”€â”€ shared(Embedding),lm_head(Linear) weight:[32100, 768]                       
â”œâ”€â”€ encoder (T5Stack)                                                           
â”‚   â”œâ”€â”€ embed_tokens (Embedding) weight:[32100, 768]                            
â”‚   â”œâ”€â”€ block (ModuleList)                                                      
â”‚   â”‚   â”œâ”€â”€ 0 (T5Block)                                                         
â”‚   â”‚   â”‚   â””â”€â”€ layer (ModuleList)                                              
â”‚   â”‚   â”‚       â”œâ”€â”€ 0 (T5LayerSelfAttention)                                    
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ SelfAttention (T5Attention)                             
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ q,k,v,o(Linear) weight:[768, 768]                   
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ relative_attention_bias (Embedding) weight:[32, 12] 
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
â”‚   â”‚   â”‚       â”‚       â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
â”‚   â”‚   â”‚       â”‚       â”‚   W_right:[1, 24] b:[24]                              
â”‚   â”‚   â”‚       â”‚       â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
â”‚   â”‚   â”‚       â”‚           W_right:[1, 768] b:[768]                            
â”‚   â”‚   â”‚       â””â”€â”€ 1 (T5LayerFF)                                               
â”‚   â”‚   â”‚           â”œâ”€â”€ DenseReluDense (T5DenseReluDense)                       
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ wi (Linear) weight:[3072, 768]                      
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ wo (Linear) weight:[768, 3072]                      
â”‚   â”‚   â”‚           â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
â”‚   â”‚   â”‚           â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
â”‚   â”‚   â”‚               â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
â”‚   â”‚   â”‚               â”‚   W_right:[1, 24] b:[24]                              
â”‚   â”‚   â”‚               â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
â”‚   â”‚   â”‚                   W_right:[1, 768] b:[768]                            
â”‚   â”‚   â””â”€â”€ 1-11(T5Block)                                                       
â”‚   â”‚       â””â”€â”€ layer (ModuleList)                                              
â”‚   â”‚           â”œâ”€â”€ 0 (T5LayerSelfAttention)                                    
â”‚   â”‚           â”‚   â”œâ”€â”€ SelfAttention (T5Attention)                             
â”‚   â”‚           â”‚   â”‚   â””â”€â”€ q,k,v,o(Linear) weight:[768, 768]                   
â”‚   â”‚           â”‚   â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
â”‚   â”‚           â”‚   â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
â”‚   â”‚           â”‚       â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
â”‚   â”‚           â”‚       â”‚   W_right:[1, 24] b:[24]                              
â”‚   â”‚           â”‚       â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
â”‚   â”‚           â”‚           W_right:[1, 768] b:[768]                            
â”‚   â”‚           â””â”€â”€ 1 (T5LayerFF)                                               
â”‚   â”‚               â”œâ”€â”€ DenseReluDense (T5DenseReluDense)                       
â”‚   â”‚               â”‚   â”œâ”€â”€ wi (Linear) weight:[3072, 768]                      
â”‚   â”‚               â”‚   â””â”€â”€ wo (Linear) weight:[768, 3072]                      
â”‚   â”‚               â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
â”‚   â”‚               â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
â”‚   â”‚                   â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
â”‚   â”‚                   â”‚   W_right:[1, 24] b:[24]                              
â”‚   â”‚                   â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
â”‚   â”‚                       W_right:[1, 768] b:[768]                            
â”‚   â””â”€â”€ final_layer_norm (T5LayerNorm) weight:[768]                             
â””â”€â”€ decoder (T5Stack)                                                           
    â”œâ”€â”€ embed_tokens (Embedding) weight:[32100, 768]                            
    â”œâ”€â”€ block (ModuleList)                                                      
    â”‚   â”œâ”€â”€ 0 (T5Block)                                                         
    â”‚   â”‚   â””â”€â”€ layer (ModuleList)                                              
    â”‚   â”‚       â”œâ”€â”€ 0 (T5LayerSelfAttention)                                    
    â”‚   â”‚       â”‚   â”œâ”€â”€ SelfAttention (T5Attention)                             
    â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ q,k,v,o(Linear) weight:[768, 768]                   
    â”‚   â”‚       â”‚   â”‚   â””â”€â”€ relative_attention_bias (Embedding) weight:[32, 12] 
    â”‚   â”‚       â”‚   â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
    â”‚   â”‚       â”‚   â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
    â”‚   â”‚       â”‚       â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
    â”‚   â”‚       â”‚       â”‚   W_right:[1, 24] b:[24]                              
    â”‚   â”‚       â”‚       â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
    â”‚   â”‚       â”‚           W_right:[1, 768] b:[768]                            
    â”‚   â”‚       â”œâ”€â”€ 1 (T5LayerCrossAttention)                                   
    â”‚   â”‚       â”‚   â”œâ”€â”€ EncDecAttention (T5Attention)                           
    â”‚   â”‚       â”‚   â”‚   â””â”€â”€ q,k,v,o(Linear) weight:[768, 768]                   
    â”‚   â”‚       â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
    â”‚   â”‚       â””â”€â”€ 2 (T5LayerFF)                                               
    â”‚   â”‚           â”œâ”€â”€ DenseReluDense (T5DenseReluDense)                       
    â”‚   â”‚           â”‚   â”œâ”€â”€ wi (Linear) weight:[3072, 768]                      
    â”‚   â”‚           â”‚   â””â”€â”€ wo (Linear) weight:[768, 3072]                      
    â”‚   â”‚           â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
    â”‚   â”‚           â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
    â”‚   â”‚               â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
    â”‚   â”‚               â”‚   W_right:[1, 24] b:[24]                              
    â”‚   â”‚               â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
    â”‚   â”‚                   W_right:[1, 768] b:[768]                            
    â”‚   â””â”€â”€ 1-11(T5Block)                                                       
    â”‚       â””â”€â”€ layer (ModuleList)                                              
    â”‚           â”œâ”€â”€ 0 (T5LayerSelfAttention)                                    
    â”‚           â”‚   â”œâ”€â”€ SelfAttention (T5Attention)                             
    â”‚           â”‚   â”‚   â””â”€â”€ q,k,v,o(Linear) weight:[768, 768]                   
    â”‚           â”‚   â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
    â”‚           â”‚   â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
    â”‚           â”‚       â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
    â”‚           â”‚       â”‚   W_right:[1, 24] b:[24]                              
    â”‚           â”‚       â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
    â”‚           â”‚           W_right:[1, 768] b:[768]                            
    â”‚           â”œâ”€â”€ 1 (T5LayerCrossAttention)                                   
    â”‚           â”‚   â”œâ”€â”€ EncDecAttention (T5Attention)                           
    â”‚           â”‚   â”‚   â””â”€â”€ q,k,v,o(Linear) weight:[768, 768]                   
    â”‚           â”‚   â””â”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
    â”‚           â””â”€â”€ 2 (T5LayerFF)                                               
    â”‚               â”œâ”€â”€ DenseReluDense (T5DenseReluDense)                       
    â”‚               â”‚   â”œâ”€â”€ wi (Linear) weight:[3072, 768]                      
    â”‚               â”‚   â””â”€â”€ wo (Linear) weight:[768, 3072]                      
    â”‚               â”œâ”€â”€ layer_norm (T5LayerNorm) weight:[768]                   
    â”‚               â””â”€â”€ low_rank_adapter (LowRankAdapter)                       
    â”‚                   â”œâ”€â”€ down_sampler (LowRankLinear) W_left:[768, 1]        
    â”‚                   â”‚   W_right:[1, 24] b:[24]                              
    â”‚                   â””â”€â”€ up_sampler (LowRankLinear) W_left:[24, 1]           
    â”‚                       W_right:[1, 768] b:[768]                            
    â””â”€â”€ final_layer_norm (T5LayerNorm) weight:[768]                             
[INFO|(OpenDelta)basemodel:617]2022-02-28 10:01:48,930 >> Trainable Ratio: 0.072496%
[INFO|(OpenDelta)basemodel:621]2022-02-28 10:01:48,932 >> Delta Parameter Ratio: 0.051143%
02/28/2022 10:02:10 - WARNING - datasets.load -   Using the latest cached version of the module from /home/hushengding/.cache/huggingface/modules/datasets_modules/datasets/super_glue/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c (last modified on Wed Feb 23 11:27:01 2022) since it couldn't be found locally at super_glue., or remotely on the Hugging Face Hub.
02/28/2022 10:02:14 - WARNING - datasets.builder -   Reusing dataset super_glue (/home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c)
02/28/2022 10:02:14 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c/cache-7922f7caa0df4b75.arrow
02/28/2022 10:02:14 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c/cache-796f3252a3b33d58.arrow
02/28/2022 10:02:37 - WARNING - datasets.builder -   Reusing dataset super_glue (/home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)
02/28/2022 10:02:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-f5cd93d67d409668.arrow
02/28/2022 10:02:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-3d15b4e33c5bd379.arrow
02/28/2022 10:02:58 - WARNING - datasets.load -   Using the latest cached version of the module from /home/hushengding/.cache/huggingface/modules/datasets_modules/datasets/super_glue/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c (last modified on Wed Feb 23 11:27:01 2022) since it couldn't be found locally at super_glue., or remotely on the Hugging Face Hub.
02/28/2022 10:02:58 - WARNING - datasets.builder -   Reusing dataset super_glue (/home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c)
02/28/2022 10:02:58 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c/cache-3fcc53f6f814009b.arrow
02/28/2022 10:02:58 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/hushengding/.cache/huggingface/datasets/super_glue/boolq/1.0.2/c4abc2e616b5d13d249d48a2daa22d11fa9fbec0ec8d43a24c24e1a624d7c78c/cache-5ad101061e51a218.arrow
/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/datasets/load.py:1650: FutureWarning: 'script_version' was renamed to 'revision' in version 1.13 and will be removed in 1.15.
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: question, passage, extra_fields, task, idx.
***** Running training *****
  Num examples = 9427
  Num Epochs = 34
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 5000
  0%|          | 0/5000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run.py", line 480, in <module>
    result = main()
  File "run.py", line 404, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/trainer.py", line 1284, in train
    tr_loss += self.training_step(model, inputs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/trainer.py", line 1789, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/trainer.py", line 1821, in compute_loss
    outputs = model(**inputs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1607, in forward
    decoder_outputs = self.decoder(
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 998, in forward
    layer_outputs = layer_module(
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 665, in forward
    cross_attention_outputs = self.layer[1](
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 580, in forward
    attention_output = self.EncDecAttention(
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 517, in forward
    attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 39.59 GiB total capacity; 14.67 GiB already allocated; 33.19 MiB free; 14.68 GiB reserved in total by PyTorch)
  0%|          | 0/5000 [00:01<?, ?it/s][33m[W 2022-02-28 10:03:12,314][0m Trial 1 failed because of the following error: FileNotFoundError(2, 'No such file or directory')[0m
Traceback (most recent call last):
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/optuna/study/_optimize.py", line 213, in _run_trial
    value_or_values = func(trial)
  File "search_single.py", line 70, in objective
    res = objective_singleseed(args, unicode = unicode, search_space_sample=search_space_sample)
  File "search_single.py", line 42, in objective_singleseed
    with open(f"{args.output_dir}/{unicode}/results.json", 'r') as fret:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs_search/lowrankadapter.superglue-boolq.t5-base/926759/results.json'
status_code 256
Traceback (most recent call last):
  File "search_single.py", line 93, in <module>
    study.optimize(partial(objective, args=args), n_trials=args.num_trials)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/optuna/study/study.py", line 400, in optimize
    _optimize(
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/optuna/study/_optimize.py", line 264, in _run_trial
    raise func_err
  File "/home/hushengding/miniconda3/envs/officialod/lib/python3.8/site-packages/optuna/study/_optimize.py", line 213, in _run_trial
    value_or_values = func(trial)
  File "search_single.py", line 70, in objective
    res = objective_singleseed(args, unicode = unicode, search_space_sample=search_space_sample)
  File "search_single.py", line 42, in objective_singleseed
    with open(f"{args.output_dir}/{unicode}/results.json", 'r') as fret:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs_search/lowrankadapter.superglue-boolq.t5-base/926759/results.json'
